#### 计算机内部只有二进制，包括字符数等

· int i = 15; // 0000000 0000000 0000000000001111

·char c = 'A'; //0000000 010000001

   十进制是人类的习惯

·计算机在输出时候，按照人类习惯输出10进制

·计算机默认接受10进制数据，自动转换为二进制数据 

   十六进制是二进制的简写

·方便人类书写与记忆

·0xff - > 1111111



​													————2018/11/3